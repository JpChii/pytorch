{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H4_CxDRhMUzr"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "Transfer learning is the technique of finding a model out there similar to our problem and fine tune it for our data.\n",
        "\n",
        "Let's see if transfer learning performs better than the models we've build earlier for Food101."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0nIDzm10MUzt",
        "outputId": "b365e3c8-201b-44e5-b4e5-07c8de7bb6ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "59nkTgG4MUzv"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We've written some modules in *modular* directory. Let's leverage them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_8j-fW3MUzw",
        "outputId": "063c6f9d-8172-4276-fbee-51de1a0b166d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 1.12.1\n",
            "torchvision version: 0.13.1\n"
          ]
        }
      ],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VlJGnLmlMUzx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch'...\n",
            "remote: Enumerating objects: 107, done.\u001b[K\n",
            "remote: Counting objects: 100% (107/107), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 107 (delta 43), reused 63 (delta 15), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (107/107), 5.73 MiB | 1.99 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from modular.src.data import data_setup\n",
        "    from modular.src.data import get_data\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/JpChii/pytorch.git\n",
        "    !mv pytorch/modular .\n",
        "    !rm -rf pytorch\n",
        "    from modular.src.data import data_setup\n",
        "    from modular.src.data import get_data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yj-Trf-MMUzx"
      },
      "source": [
        "## Get data\n",
        "\n",
        "Let's download the data before we start transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wZVse7yMUzy",
        "outputId": "181611c8-3518-4569-93de-a34f4bc419f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image download directory: data/pizza_steak_sushi\n",
            "Zip path: data/pizza_steak_sushi.zip\n",
            "data directory exists\n",
            "data/pizza_steak_sushi directory exists\n",
            "Downloading data fromhttps://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip...\n",
            "Unzip data\n",
            "data/pizza_steak_sushi.zip cleand after unzip\n"
          ]
        }
      ],
      "source": [
        "images_dir = get_data.get_data(\n",
        "    request_url=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "    data_path=\"data/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHfqqv6tMUzy",
        "outputId": "44e0fc90-778b-4beb-b06b-81435a3a4ced"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi/train'),\n",
              " PosixPath('data/pizza_steak_sushi/test'))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Setup train and test directories\n",
        "from pathlib import Path\n",
        "train_dir = Path(f\"data/{images_dir}/train\")\n",
        "test_dir = Path(f\"data/{images_dir}/test\")\n",
        "train_dir, test_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CkFM_JiJMUzz"
      },
      "source": [
        "## Creating Datasets and DataLoaders\n",
        "\n",
        "We'll use our modular imports to create datasets and dataloaders.\n",
        "\n",
        "> Note: As of torchvision v0.13+ there's an update on how transforms can be create using `torchvision.models`. The previous method is manual and current method is auto.\n",
        "\n",
        "While using pretrained models, we've to make sure the custom data is in the same form as the original data used to train the pretrained model.\n",
        "\n",
        "Prior to torchvision v0.13+, to create a transform for pretrained model in `torchvision.models`.\n",
        "The documentation stated below:\n",
        "\n",
        "> All pre-trained model expect input images normalized in the same way. i.e mini-batches of 3-channel RGB images of shape (3 * H * W) where H and W expected to be at least 224.\n",
        "\n",
        "> The images have to be loaded in to a range if [0, 1] and then normalized using mean=[0485, 0.456, 0.406] and std=[0.229, 0.224, 0..225]."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty-1ijdgMUzz"
      },
      "source": [
        "### Manual transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "184fzJlKMUzz",
        "outputId": "84f02270-88f2-4fe5-b2b4-c28373966edf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
              "    ToTensor()\n",
              "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating manual transform\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)), # Resize images to 224, 224\n",
        "    transforms.ToTensor(), # Convert to tensor and between [0, 1]\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    )\n",
        "])\n",
        "manual_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdHqY3Z8MUz0",
        "outputId": "c7967e1b-3027-40db-ebf5-7b41da4e0b49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7fea2f5ff9a0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7fea33cc72b0>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's create the dataloder\n",
        "BATCH_SIZE=32\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk42pKLOMUz0"
      },
      "source": [
        "### Auto transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2S1mJHVgMUz1"
      },
      "outputs": [],
      "source": [
        "# Let's load the weights\n",
        "# Assume we want to use EfficientNet_B0 and it's best version(DEFAULT)\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dxU7RU_MUz1",
        "outputId": "7ad08e8c-a9ea-4097-c1aa-4a7a5af80bb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[256]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BICUBIC\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can get the transform from weights\n",
        "auto_transforms = weights.transforms()\n",
        "auto_transforms"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iyLucxYUMUz2"
      },
      "source": [
        "Notice how `auto_transforms` is similar to `manual_transforms`. The only difference is that `auto_transforms` came with the model architecture we chose and `manual_tansforms` is create by hand.\n",
        "\n",
        "With `auto_transforms` we can ensure that our data goes through same transformation as the original data pretrained model use but it lacks customization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khzVlsbFMUz2",
        "outputId": "167f7bf8-4a78-4b51-a831-8a38cf6c95b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7fea33cc7280>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7fea33cc77f0>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's create the dataloaders with atuo transforms\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=auto_transforms,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wavso6OMUz2"
      },
      "source": [
        "## Getting a pretrained model\n",
        "\n",
        "There are lot's of versions for a single pretrained model. The suffix number start from smallest to largest. Compute and performance improves from smallest to largest.\n",
        "\n",
        "The model selection depends on **perforamance vs speed vs size**.\n",
        "\n",
        "For this problem we'll use EffieicentNet_B0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PqcmydNUMUz2"
      },
      "outputs": [],
      "source": [
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pl4yK5hnMUz3"
      },
      "source": [
        "`efficientnet_b0` comes in three parts:\n",
        "\n",
        "1. `features`: A colection of convolutional layers and other various activation layers to learn a base reperesentation of vision data(this base reperesentaiton/collection of layers is often referred to as features or feature extractor)\n",
        "2. `avgpool`: Take the average of the output of the `features` layers(s) and turns it into a **feature vector**.\n",
        "3. `classifier`: Turns the feature vector into a vector with same dimensionality as the number of required output classes."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vwFk_b1lMUz3"
      },
      "source": [
        "### Model summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZGlHTpEMUz3",
        "outputId": "519f2b0a-2960-4dbe-cfae-0cf7afa2de56"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   True\n",
              "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   True\n",
              "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   True\n",
              "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   864                  True\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   64                   True\n",
              "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
              "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\n",
              "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     6,004                True\n",
              "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n",
              "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     15,350               True\n",
              "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     31,290               True\n",
              "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     37,130               True\n",
              "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n",
              "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n",
              "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    126,004              True\n",
              "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n",
              "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n",
              "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      262,492              True\n",
              "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n",
              "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n",
              "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n",
              "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   True\n",
              "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      717,232              True\n",
              "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   True\n",
              "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     409,600              True\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     2,560                True\n",
              "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
              "├─Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   True\n",
              "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
              "│    └─Linear (1)                                            [32, 1280]           [32, 1000]           1,281,000            True\n",
              "============================================================================================================================================\n",
              "Total params: 5,288,548\n",
              "Trainable params: 5,288,548\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 12.35\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3452.35\n",
              "Params size (MB): 21.15\n",
              "Estimated Total Size (MB): 3492.77\n",
              "============================================================================================================================================"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(\n",
        "    model=model,\n",
        "    input_size=(32, 3, 224, 224),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7PYvNyrxMUz4"
      },
      "source": [
        "That's a big model and has lots of parameters(pretrained weights) to recogonize different patterns in our data.\n",
        "\n",
        "TinyVGG 8,083 --> EffNetB0 5,288,548 _ 65x.\n",
        "\n",
        "Will this have better performance?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oOlcoP3UMUz4"
      },
      "source": [
        "### Freezing the base model and changing the output layer to suit our needs\n",
        "\n",
        "The process of transfer learning usually goes: freeze some base layers of a pretrained model(typically the `features` section) and then adjust the output layers (also called heaad/classifier layers) to suit the problems needs.\n",
        "\n",
        "In our case:\n",
        "*The original `torchvision.models.efficientnet_b0()` comes outwith `out_features=1000` because there are 1000 classes in ImageNet, the dataset it was trained on. For our current problem. We have only three classes.\n",
        "\n",
        "***Freeze*** -- Retain weights of feature layers and use the patterns it learned from original dataset(ImageNet in efficientnet_b0) and use them as backbone for our problem.\n",
        "\n",
        "Let's freeze the features and customize the classifier."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZcCzUnNOCdg"
      },
      "source": [
        "#### Freeze `features` section\n",
        "\n",
        "PyTorch tracks gradients only when their `requires_grad=True` is set. To Freeze let's set `requires_grad=False`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DkehyfCSOkJM"
      },
      "outputs": [],
      "source": [
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "  param.requires_grad=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvyGF1ZSOsC4",
        "outputId": "c4996bfb-433a-4531-d74a-ed2628c7fb9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   False\n",
              "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
              "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
              "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
              "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
              "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
              "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
              "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
              "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
              "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
              "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
              "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
              "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
              "├─Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   False\n",
              "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
              "│    └─Linear (1)                                            [32, 1280]           [32, 1000]           (1,281,000)          False\n",
              "============================================================================================================================================\n",
              "Total params: 5,288,548\n",
              "Trainable params: 0\n",
              "Non-trainable params: 5,288,548\n",
              "Total mult-adds (G): 12.35\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3452.35\n",
              "Params size (MB): 21.15\n",
              "Estimated Total Size (MB): 3492.77\n",
              "============================================================================================================================================"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(\n",
        "    model=model,\n",
        "    input_size=(32, 3, 224, 224),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DFaOZdRdOv_u"
      },
      "source": [
        "See all layers trainable field has turned to `False` in model summary."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "STT1uKXHO4zG"
      },
      "source": [
        "#### Modifying classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aod246RNO8Qa",
        "outputId": "5b229146-a0d1-4505-f3d5-a92392f8fd14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.2, inplace=True)\n",
              "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Current classifier\n",
        "model.classifier"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bITNIMLRO-aR"
      },
      "source": [
        "* Let's retain the regularization to avoid overfitting model. \n",
        "\n",
        "* Droput randomnly drops connections between neural networks forcing it to learn all paths.\n",
        "\n",
        "* In general we're training multiple models with dropouts.\n",
        "\n",
        "* `in_features` will reamin `1280` as we get it from `feature` and change `out_features` to `3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CTr_VuwePXiF"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Get the length of class names (one output for each item)\n",
        "output_shape = len(class_names)\n",
        "\n",
        "# Recreate classifier layer and send it to the target device\n",
        "model.classifier = nn.Sequential(\n",
        "  nn.Dropout(\n",
        "    p=0.2, \n",
        "    inplace=True\n",
        "  ),\n",
        "  nn.Linear(\n",
        "    in_features=1280, \n",
        "    out_features=3, \n",
        "    bias=True\n",
        "  ),\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnyYawBaQFCn",
        "outputId": "4317f4cf-357f-4354-ce99-93d9ffac1133"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
              "├─Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
              "│    │    └─SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
              "│    └─Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
              "│    └─Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
              "│    └─Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
              "│    │    └─MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
              "│    └─Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
              "│    │    └─MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "│    │    └─MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "│    └─Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
              "│    │    └─MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "│    │    └─MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "│    └─Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
              "│    │    └─MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    │    └─MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    │    └─MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "│    └─Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
              "│    └─Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
              "│    │    └─SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
              "├─Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n",
              "│    └─Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
              "│    └─Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n",
              "============================================================================================================================================\n",
              "Total params: 4,011,391\n",
              "Trainable params: 3,843\n",
              "Non-trainable params: 4,007,548\n",
              "Total mult-adds (G): 12.31\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3452.09\n",
              "Params size (MB): 16.05\n",
              "Estimated Total Size (MB): 3487.41\n",
              "============================================================================================================================================"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's check the model summary\n",
        "summary(\n",
        "    model=model,\n",
        "    input_size=(32, 3, 224, 224),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SeDXCIWVQJNV"
      },
      "source": [
        "Inference from summary:\n",
        "* Classifier is Trainable and other layers are not \n",
        "* Output shape of classifier (32, 1000) --> (32, 3)\n",
        "* Less trainable parameters\n",
        "* Lesser params lesser compute🔥"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oA5r9TgXVm4Q"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yyaBdJkHVqDz"
      },
      "outputs": [],
      "source": [
        "# loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LLgu7BspVzU3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dded46f923f946e38e1d042414d1b831",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "UnboundLocalError",
          "evalue": "local variable 'test_accuracy' referenced before assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb Cell 37\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb#Y162sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodular\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m engine\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb#Y162sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Setup training and save resuls\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb#Y162sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m results \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mtrain_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb#Y162sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb#Y162sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     train_dataloder\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb#Y162sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     test_dataloader\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb#Y162sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb#Y162sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb#Y162sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb#Y162sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jayaprakashsivagami/Documents/Tech/ML/pytorch/computer_vision/3-transfer-learning.ipynb#Y162sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/Tech/ML/pytorch/computer_vision/modular/src/model/engine.py:191\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloder, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(epochs)):\n\u001b[1;32m    183\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train_step(\n\u001b[1;32m    184\u001b[0m         model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    185\u001b[0m         dataloader\u001b[39m=\u001b[39mtrain_dataloder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m         device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m    189\u001b[0m     )\n\u001b[0;32m--> 191\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test_step(\n\u001b[1;32m    192\u001b[0m         model\u001b[39m=\u001b[39;49mmodel, dataloader\u001b[39m=\u001b[39;49mtest_dataloader, loss_fn\u001b[39m=\u001b[39;49mloss_fn, device\u001b[39m=\u001b[39;49mdevice\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    195\u001b[0m     \u001b[39m# Print out what's happening\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    197\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest_acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/Tech/ML/pytorch/computer_vision/modular/src/model/engine.py:125\u001b[0m, in \u001b[0;36mtest_step\u001b[0;34m(model, dataloader, loss_fn, device)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# Adjust metrics to get average loss and accuracy per batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m test_loss \u001b[39m=\u001b[39m test_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n\u001b[0;32m--> 125\u001b[0m test_accuracy \u001b[39m=\u001b[39m test_accuracy \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n\u001b[1;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m test_loss, test_accuracy\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'test_accuracy' referenced before assignment"
          ]
        }
      ],
      "source": [
        "# Train the classifier in model_0 using engine from modular\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Import engine\n",
        "from modular.src.model import engine\n",
        "\n",
        "# Setup training and save resuls\n",
        "results = engine.train_model(\n",
        "    model=model,\n",
        "    train_dataloder=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=5,\n",
        "    device=device,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyAarAb4XhUB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4_CxDRhMUzr"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "Transfer learning is the technique of finding a model out there similar to our problem and fine tune it for our data.\n",
        "\n",
        "Let's see if transfer learning performs better than the models we've build earlier for Food101."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0nIDzm10MUzt",
        "outputId": "b365e3c8-201b-44e5-b4e5-07c8de7bb6ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "# Device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59nkTgG4MUzv"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We've written some modules in *modular* directory. Let's leverage them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_8j-fW3MUzw",
        "outputId": "063c6f9d-8172-4276-fbee-51de1a0b166d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] torch/torchvision versions not as required, installing nightly versions.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cu113\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "torch version: 2.0.1+cu118\n",
            "torchvision version: 0.15.2+cu118\n"
          ]
        }
      ],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "VlJGnLmlMUzx"
      },
      "outputs": [],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from modular.src.data import data_setup\n",
        "    from modular.src.data import get_data\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/JpChii/pytorch.git\n",
        "    !mv pytorch/modular .\n",
        "    !rm -rf pytorch\n",
        "    from modular.src.data import data_setup\n",
        "    from modular.src.data import get_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj-Trf-MMUzx"
      },
      "source": [
        "## Get data\n",
        "\n",
        "Let's download the data before we start transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wZVse7yMUzy",
        "outputId": "181611c8-3518-4569-93de-a34f4bc419f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image download directory: data/pizza_steak_sushi\n",
            "Zip path: data/pizza_steak_sushi.zip\n",
            "data directory exists\n",
            "data/pizza_steak_sushi directory exists\n",
            "Downloading data fromhttps://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip...\n",
            "Unzip data\n",
            "data/pizza_steak_sushi.zip cleand after unzip\n"
          ]
        }
      ],
      "source": [
        "images_dir = get_data.get_data(\n",
        "    request_url=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "    data_path=\"data/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHfqqv6tMUzy",
        "outputId": "44e0fc90-778b-4beb-b06b-81435a3a4ced"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi/train'),\n",
              " PosixPath('data/pizza_steak_sushi/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# Setup train and test directories\n",
        "from pathlib import Path\n",
        "train_dir = Path(f\"data/{images_dir}/train\")\n",
        "test_dir = Path(f\"data/{images_dir}/test\")\n",
        "train_dir, test_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkFM_JiJMUzz"
      },
      "source": [
        "## Creating Datasets and DataLoaders\n",
        "\n",
        "We'll use our modular imports to create datasets and dataloaders.\n",
        "\n",
        "> Note: As of torchvision v0.13+ there's an update on how transforms can be create using `torchvision.models`. The previous method is manual and current method is auto.\n",
        "\n",
        "While using pretrained models, we've to make sure the custom data is in the same form as the original data used to train the pretrained model.\n",
        "\n",
        "Prior to torchvision v0.13+, to create a transform for pretrained model in `torchvision.models`.\n",
        "The documentation stated below:\n",
        "\n",
        "> All pre-trained model expect input images normalized in the same way. i.e mini-batches of 3-channel RGB images of shape (3 * H * W) where H and W expected to be at least 224.\n",
        "\n",
        "> The images have to be loaded in to a range if [0, 1] and then normalized using mean=[0485, 0.456, 0.406] and std=[0.229, 0.224, 0..225]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty-1ijdgMUzz"
      },
      "source": [
        "### Manual transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "184fzJlKMUzz",
        "outputId": "84f02270-88f2-4fe5-b2b4-c28373966edf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
              "    ToTensor()\n",
              "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "# Creating manual transform\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)), # Resize images to 224, 224\n",
        "    transforms.ToTensor(), # Convert to tensor and between [0, 1]\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    )\n",
        "])\n",
        "manual_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdHqY3Z8MUz0",
        "outputId": "c7967e1b-3027-40db-ebf5-7b41da4e0b49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7fa258bdf4f0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7fa258bde320>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "# Let's create the dataloder\n",
        "BATCH_SIZE=32\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk42pKLOMUz0"
      },
      "source": [
        "### Auto transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "2S1mJHVgMUz1"
      },
      "outputs": [],
      "source": [
        "# Let's load the weights\n",
        "# Assume we want to use EfficientNet_B0 and it's best version(DEFAULT)\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dxU7RU_MUz1",
        "outputId": "7ad08e8c-a9ea-4097-c1aa-4a7a5af80bb1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[224]\n",
              "    resize_size=[256]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BICUBIC\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# We can get the transform from weights\n",
        "auto_transforms = weights.transforms()\n",
        "auto_transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyLucxYUMUz2"
      },
      "source": [
        "Notice how `auto_transforms` is similar to `manual_transforms`. The only difference is that `auto_transforms` came with the model architecture we chose and `manual_tansforms` is create by hand.\n",
        "\n",
        "With `auto_transforms` we can ensure that our data goes through same transformation as the original data pretrained model use but it lacks customization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khzVlsbFMUz2",
        "outputId": "167f7bf8-4a78-4b51-a831-8a38cf6c95b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7fa258bdcb80>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7fa258bdf9a0>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "# Let's create the dataloaders with atuo transforms\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=auto_transforms,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Wavso6OMUz2"
      },
      "source": [
        "## Getting a pretrained model\n",
        "\n",
        "There are lot's of versions for a single pretrained model. The suffix number start from smallest to largest. Compute and performance improves from smallest to largest.\n",
        "\n",
        "The model selection depends on **perforamance vs speed vs size**.\n",
        "\n",
        "For this problem we'll use EffieicentNet_B0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "PqcmydNUMUz2"
      },
      "outputs": [],
      "source": [
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl4yK5hnMUz3"
      },
      "source": [
        "`efficientnet_b0` comes in three parts:\n",
        "\n",
        "1. `features`: A colection of convolutional layers and other various activation layers to learn a base reperesentation of vision data(this base reperesentaiton/collection of layers is often referred to as features or feature extractor)\n",
        "2. `avgpool`: Take the average of the output of the `features` layers(s) and turns it into a **feature vector**.\n",
        "3. `classifier`: Turns the feature vector into a vector with same dimensionality as the number of required output classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwFk_b1lMUz3"
      },
      "source": [
        "### Model summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZGlHTpEMUz3",
        "outputId": "519f2b0a-2960-4dbe-cfae-0cf7afa2de56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   True\n",
              "â”œâ”€Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   True\n",
              "â”‚    â””â”€Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   True\n",
              "â”‚    â”‚    â””â”€Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   864                  True\n",
              "â”‚    â”‚    â””â”€BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   64                   True\n",
              "â”‚    â”‚    â””â”€SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
              "â”‚    â””â”€Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   True\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\n",
              "â”‚    â””â”€Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   True\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     6,004                True\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n",
              "â”‚    â””â”€Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   True\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     15,350               True\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     31,290               True\n",
              "â”‚    â””â”€Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   True\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     37,130               True\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n",
              "â”‚    â”‚    â””â”€MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n",
              "â”‚    â””â”€Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   True\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    126,004              True\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n",
              "â”‚    â”‚    â””â”€MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n",
              "â”‚    â””â”€Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   True\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      262,492              True\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n",
              "â”‚    â”‚    â””â”€MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n",
              "â”‚    â”‚    â””â”€MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n",
              "â”‚    â””â”€Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   True\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      717,232              True\n",
              "â”‚    â””â”€Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   True\n",
              "â”‚    â”‚    â””â”€Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     409,600              True\n",
              "â”‚    â”‚    â””â”€BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     2,560                True\n",
              "â”‚    â”‚    â””â”€SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
              "â”œâ”€AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
              "â”œâ”€Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   True\n",
              "â”‚    â””â”€Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
              "â”‚    â””â”€Linear (1)                                            [32, 1280]           [32, 1000]           1,281,000            True\n",
              "============================================================================================================================================\n",
              "Total params: 5,288,548\n",
              "Trainable params: 5,288,548\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 12.35\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3452.35\n",
              "Params size (MB): 21.15\n",
              "Estimated Total Size (MB): 3492.77\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "summary(\n",
        "    model=model,\n",
        "    input_size=(32, 3, 224, 224),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PYvNyrxMUz4"
      },
      "source": [
        "That's a big model and has lots of parameters(pretrained weights) to recogonize different patterns in our data.\n",
        "\n",
        "TinyVGG 8,083 --> EffNetB0 5,288,548 _ 65x.\n",
        "\n",
        "Will this have better performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOlcoP3UMUz4"
      },
      "source": [
        "### Freezing the base model and changing the output layer to suit our needs\n",
        "\n",
        "The process of transfer learning usually goes: freeze some base layers of a pretrained model(typically the `features` section) and then adjust the output layers (also called heaad/classifier layers) to suit the problems needs.\n",
        "\n",
        "In our case:\n",
        "*The original `torchvision.models.efficientnet_b0()` comes outwith `out_features=1000` because there are 1000 classes in ImageNet, the dataset it was trained on. For our current problem. We have only three classes.\n",
        "\n",
        "***Freeze*** -- Retain weights of feature layers and use the patterns it learned from original dataset(ImageNet in efficientnet_b0) and use them as backbone for our problem.\n",
        "\n",
        "Let's freeze the features and customize the classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Freeze `features` section\n",
        "\n",
        "PyTorch tracks gradients only when their `requires_grad=True` is set. To Freeze let's set `requires_grad=False`"
      ],
      "metadata": {
        "id": "2ZcCzUnNOCdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "  param.requires_grad=False"
      ],
      "metadata": {
        "id": "DkehyfCSOkJM"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(\n",
        "    model=model,\n",
        "    input_size=(32, 3, 224, 224),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvyGF1ZSOsC4",
        "outputId": "c4996bfb-433a-4531-d74a-ed2628c7fb9e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   False\n",
              "â”œâ”€Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
              "â”‚    â””â”€Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
              "â”‚    â”‚    â””â”€Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
              "â”‚    â”‚    â””â”€BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
              "â”‚    â”‚    â””â”€SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
              "â”‚    â””â”€Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
              "â”‚    â””â”€Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
              "â”‚    â””â”€Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
              "â”‚    â””â”€Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "â”‚    â”‚    â””â”€MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "â”‚    â””â”€Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "â”‚    â”‚    â””â”€MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "â”‚    â””â”€Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "â”‚    â”‚    â””â”€MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "â”‚    â”‚    â””â”€MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "â”‚    â””â”€Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
              "â”‚    â””â”€Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
              "â”‚    â”‚    â””â”€Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
              "â”‚    â”‚    â””â”€BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
              "â”‚    â”‚    â””â”€SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
              "â”œâ”€AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
              "â”œâ”€Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   False\n",
              "â”‚    â””â”€Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
              "â”‚    â””â”€Linear (1)                                            [32, 1280]           [32, 1000]           (1,281,000)          False\n",
              "============================================================================================================================================\n",
              "Total params: 5,288,548\n",
              "Trainable params: 0\n",
              "Non-trainable params: 5,288,548\n",
              "Total mult-adds (G): 12.35\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3452.35\n",
              "Params size (MB): 21.15\n",
              "Estimated Total Size (MB): 3492.77\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See all layers trainable field has turned to `False` in model summary."
      ],
      "metadata": {
        "id": "DFaOZdRdOv_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Modifying classifier"
      ],
      "metadata": {
        "id": "STT1uKXHO4zG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Current classifier\n",
        "model.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aod246RNO8Qa",
        "outputId": "5b229146-a0d1-4505-f3d5-a92392f8fd14"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.2, inplace=True)\n",
              "  (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let's retain the regularization to avoid overfitting model. \n",
        "\n",
        "* Droput randomnly drops connections between neural networks forcing it to learn all paths.\n",
        "\n",
        "* In general we're training multiple models with dropouts.\n",
        "\n",
        "* `in_features` will reamin `1280` as we get it from `feature` and change `out_features` to `3`."
      ],
      "metadata": {
        "id": "bITNIMLRO-aR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Get the length of class names (one output for each item)\n",
        "output_shape = len(class_names)\n",
        "\n",
        "# Recreate classifier layer and send it to the target device\n",
        "model.classifier = nn.Sequential(\n",
        "  nn.Dropout(\n",
        "    p=0.2, \n",
        "    inplace=True\n",
        "  ),\n",
        "  nn.Linear(\n",
        "    in_features=1280, \n",
        "    out_features=3, \n",
        "    bias=True\n",
        "  ),\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "CTr_VuwePXiF"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check the model summary\n",
        "summary(\n",
        "    model=model,\n",
        "    input_size=(32, 3, 224, 224),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnyYawBaQFCn",
        "outputId": "4317f4cf-357f-4354-ce99-93d9ffac1133"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n",
              "â”œâ”€Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n",
              "â”‚    â””â”€Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n",
              "â”‚    â”‚    â””â”€Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n",
              "â”‚    â”‚    â””â”€BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n",
              "â”‚    â”‚    â””â”€SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n",
              "â”‚    â””â”€Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n",
              "â”‚    â””â”€Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n",
              "â”‚    â””â”€Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n",
              "â”‚    â””â”€Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "â”‚    â”‚    â””â”€MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n",
              "â”‚    â””â”€Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "â”‚    â”‚    â””â”€MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n",
              "â”‚    â””â”€Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n",
              "â”‚    â”‚    â””â”€MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "â”‚    â”‚    â””â”€MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "â”‚    â”‚    â””â”€MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n",
              "â”‚    â””â”€Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n",
              "â”‚    â”‚    â””â”€MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n",
              "â”‚    â””â”€Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n",
              "â”‚    â”‚    â””â”€Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n",
              "â”‚    â”‚    â””â”€BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n",
              "â”‚    â”‚    â””â”€SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n",
              "â”œâ”€AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n",
              "â”œâ”€Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n",
              "â”‚    â””â”€Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n",
              "â”‚    â””â”€Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n",
              "============================================================================================================================================\n",
              "Total params: 4,011,391\n",
              "Trainable params: 3,843\n",
              "Non-trainable params: 4,007,548\n",
              "Total mult-adds (G): 12.31\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3452.09\n",
              "Params size (MB): 16.05\n",
              "Estimated Total Size (MB): 3487.41\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference from summary:\n",
        "* Classifier is Trainable and other layers are not \n",
        "* Output shape of classifier (32, 1000) --> (32, 3)\n",
        "* Less trainable parameters\n",
        "* Lesser params lesser computeðŸ”¥"
      ],
      "metadata": {
        "id": "SeDXCIWVQJNV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ],
      "metadata": {
        "id": "oA5r9TgXVm4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "yyaBdJkHVqDz"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timer"
      ],
      "metadata": {
        "id": "nimhGh3cWIbY",
        "outputId": "b9db2d08-d325-4c5f-cbe4-23af86a3335a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: timer in /usr/local/lib/python3.10/dist-packages (0.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LLgu7BspVzU3"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GyAarAb4XhUB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}